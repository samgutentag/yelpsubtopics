{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from glob import glob\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_marker(text=''):\n",
    "    print('[{}] {}'.format(datetime.datetime.now().time(), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import matplotlib\n",
    "font = {'size' : 50}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "TITLE_FONT_SIZE = 25\n",
    "LABEL_FONT_SIZE = 15\n",
    "TICK_FONT_SIZE  = 15\n",
    "\n",
    "FIG_SIZE = (15,6)\n",
    "DO_WRITE_CHARTS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Review Data for Arizona Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:37:36.271084] Loading Review Data...\n",
      "[02:37:36.387878] Reading 1 of 1 ../clean_data/az_restaurant_reviews.csv...\n",
      "[02:37:47.383567] merging to dataframe...\n",
      "[02:37:50.782648] reseting index...\n",
      "[02:37:50.801102] Complete!\n"
     ]
    }
   ],
   "source": [
    "time_marker(text='Loading Review Data...')\n",
    "\n",
    "reviews = pd.DataFrame()\n",
    "file_path_slug = '../clean_data/az_restaurant_reviews.csv'\n",
    "file_list = glob(file_path_slug)\n",
    "\n",
    "# Chunk Settings\n",
    "chunks = list()\n",
    "chunksize = 10000\n",
    "for ii, file in enumerate(sorted(file_list)):\n",
    "    time_marker('Reading {} of {} {}...'.format(ii+1, len(file_list), file))\n",
    "    num_chunks = math.ceil(sum(1 for row in open(file, 'r'))/chunksize)\n",
    "    format_width = len(str(num_chunks))\n",
    "\n",
    "    # import file in chunks\n",
    "    for jj, chunk in enumerate(pd.read_csv(file, chunksize=chunksize, iterator=True, index_col=0, parse_dates=['date'])):\n",
    "\n",
    "        \n",
    "        # append chunk to chunks list\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "time_marker(text='merging to dataframe...')\n",
    "reviews = pd.concat(chunks)\n",
    "\n",
    "time_marker('reseting index...')\n",
    "reviews.reset_index(inplace=True, drop=True)\n",
    "time_marker(text='Complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.dropna(how='any', inplace=True)\n",
    "reviews.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews['cool'] = reviews['cool'].astype('int')\n",
    "reviews['funny'] = reviews['funny'].astype('int')\n",
    "reviews['stars'] = reviews['stars'].astype('int')\n",
    "reviews['useful'] = reviews['useful'].astype('int')\n",
    "reviews['review_len'] = reviews['review_len'].astype('int')\n",
    "reviews['is_fast_food'] = reviews['is_fast_food'].astype('int')\n",
    "\n",
    "reviews['date'] = pd.to_datetime(reviews['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_fast_food</th>\n",
       "      <th>review_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JlNeaOymdVbE6_bubqjohg</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08-09</td>\n",
       "      <td>0</td>\n",
       "      <td>BF0ANB54sc_f-3_howQBCg</td>\n",
       "      <td>1</td>\n",
       "      <td>we always go to the chevo's in chandler which ...</td>\n",
       "      <td>3</td>\n",
       "      <td>ssuXFjkH4neiBgwv-oN4IA</td>\n",
       "      <td>0</td>\n",
       "      <td>422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08-09</td>\n",
       "      <td>0</td>\n",
       "      <td>DbLUpPT61ykLTakknCF9CQ</td>\n",
       "      <td>1</td>\n",
       "      <td>this place is always so dirty and grimy been t...</td>\n",
       "      <td>6</td>\n",
       "      <td>ssuXFjkH4neiBgwv-oN4IA</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S-oLPRdhlyL5HAknBKTUcQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>0</td>\n",
       "      <td>z_mVLygzPn8uHp63SSCErw</td>\n",
       "      <td>4</td>\n",
       "      <td>holy portion sizes! you get a lot of bang for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>MzEnYCyZlRYQRISNMXTWIg</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny               review_id  \\\n",
       "0  JlNeaOymdVbE6_bubqjohg     0 2014-08-09      0  BF0ANB54sc_f-3_howQBCg   \n",
       "1  0Rni7ocMC_Lg2UH0lDeKMQ     0 2014-08-09      0  DbLUpPT61ykLTakknCF9CQ   \n",
       "2  S-oLPRdhlyL5HAknBKTUcQ     0 2017-11-30      0  z_mVLygzPn8uHp63SSCErw   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      1  we always go to the chevo's in chandler which ...       3   \n",
       "1      1  this place is always so dirty and grimy been t...       6   \n",
       "2      4  holy portion sizes! you get a lot of bang for ...       0   \n",
       "\n",
       "                  user_id  is_fast_food  review_len  \n",
       "0  ssuXFjkH4neiBgwv-oN4IA             0         422  \n",
       "1  ssuXFjkH4neiBgwv-oN4IA             0         111  \n",
       "2  MzEnYCyZlRYQRISNMXTWIg             0         130  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 495893 entries, 0 to 495892\n",
      "Data columns (total 11 columns):\n",
      "business_id     495893 non-null object\n",
      "cool            495893 non-null int64\n",
      "date            495893 non-null datetime64[ns]\n",
      "funny           495893 non-null int64\n",
      "review_id       495893 non-null object\n",
      "stars           495893 non-null int64\n",
      "text            495893 non-null object\n",
      "useful          495893 non-null int64\n",
      "user_id         495893 non-null object\n",
      "is_fast_food    495893 non-null int64\n",
      "review_len      495893 non-null int64\n",
      "dtypes: datetime64[ns](1), int64(6), object(4)\n",
      "memory usage: 41.6+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split up by Review Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_star_reviews    65786\n",
      "two_star_reviews    47729\n",
      "three_star_reviews  59605\n",
      "four_star_reviews  124348\n",
      "five_star_reviews  198425\n"
     ]
    }
   ],
   "source": [
    "one_star_reviews   = reviews[reviews.stars == 1].copy()\n",
    "two_star_reviews   = reviews[reviews.stars == 2].copy()\n",
    "three_star_reviews = reviews[reviews.stars == 3].copy()\n",
    "four_star_reviews  = reviews[reviews.stars == 4].copy()\n",
    "five_star_reviews  = reviews[reviews.stars == 5].copy()\n",
    "\n",
    "print('one_star_reviews    {:d}'.format(one_star_reviews.shape[0]))\n",
    "print('two_star_reviews    {:d}'.format(two_star_reviews.shape[0]))\n",
    "print('three_star_reviews  {:d}'.format(three_star_reviews.shape[0]))\n",
    "print('four_star_reviews  {:d}'.format(four_star_reviews.shape[0]))\n",
    "print('five_star_reviews  {:d}'.format(five_star_reviews.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Key Terms in Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this is my first post. i'm also in the service industry. this restaurant experience was by far the worst i've ever experienced in fine dining. i'm sure it will be closed soon. service and food was way below par. we had a table of six two birthdays which we called ahead for. it took them 15-20 min to bring our drinks which the server got wrong. took them another 20-30 min to f-up one side salad order. also our dinner orders were wrong. i asked for a 6oz fliet and got salmon. i got my entree when everyone was finished and they never apologized. we had to send a smoke signal to our server. ther restaurant was only a 1/4 full of tables that were busy on a friday . food was not worth the money and never acknowledged our two birthdays. we brought all complaints to management and he looked at us like he could care less. we paid our bill like troopers no apologies from management no tip which is not in my very being. soo disappointed. i'm trying to get our money back. it's just principle. i could have had a better quicker meal at taco bell.\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_one_star = one_star_reviews.iloc[10219,:].text\n",
    "\n",
    "sample_one_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_one_star = nlp(sample_one_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: this is my first post.\n",
      "Sentence 2: i'm also in the service industry.\n",
      "Sentence 3: this restaurant experience was by far the worst i've ever experienced in fine dining.\n",
      "Sentence 4: i'm sure it will be closed soon.\n",
      "Sentence 5: service and food was way below par.\n",
      "Sentence 6: we had a table of six two birthdays which we called ahead for.\n",
      "Sentence 7: it took them 15-20 min to bring our drinks which the server got wrong.\n",
      "Sentence 8: took them another 20-30 min to f-up one side salad order.\n",
      "Sentence 9: also our dinner orders were wrong.\n",
      "Sentence 10: i asked for a 6oz fliet and got salmon.\n",
      "Sentence 11: i got my entree when everyone was finished and they never apologized.\n",
      "Sentence 12: we had to send a smoke signal to our server.\n",
      "Sentence 13: ther restaurant was only a 1/4 full of tables that were busy on a friday .\n",
      "Sentence 14: food was not worth the money and never acknowledged our two birthdays.\n",
      "Sentence 15: we brought all complaints to management and he looked at us like he could care less.\n",
      "Sentence 16: we paid our bill like troopers no apologies from management no tip which is not in my very being.\n",
      "Sentence 17: soo disappointed.\n",
      "Sentence 18: i'm trying to get our money back.\n",
      "Sentence 19: it's just principle.\n",
      "Sentence 20: i could have had a better quicker meal at taco bell.\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_one_star.sents):\n",
    "    print('Sentence {}: {}'.format(num + 1, sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_text = [token.orth_ for token in parsed_one_star]\n",
    "token_pos = [token.pos_ for token in parsed_one_star]\n",
    "\n",
    "pos = pd.DataFrame({'token_text': token_text, 'part_of_speech': token_pos})\n",
    "\n",
    "nouns_1star = pos[pos.part_of_speech =='NOUN'].copy()\n",
    "adjs_1star = pos[pos.part_of_speech =='ADJ'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>token_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>first</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>fine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>sure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>our</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>our</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>wrong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>fliet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>our</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>busy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>our</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>our</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>which</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>my</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>very</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>disappointed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>our</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>principle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>quicker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>ADJ</td>\n",
       "      <td>taco</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    part_of_speech    token_text\n",
       "2              ADJ            my\n",
       "3              ADJ         first\n",
       "21             ADJ         worst\n",
       "27             ADJ          fine\n",
       "32             ADJ          sure\n",
       "55             ADJ         which\n",
       "70             ADJ           our\n",
       "72             ADJ         which\n",
       "76             ADJ         wrong\n",
       "95             ADJ           our\n",
       "99             ADJ         wrong\n",
       "107            ADJ         fliet\n",
       "114            ADJ            my\n",
       "133            ADJ           our\n",
       "142            ADJ          full\n",
       "145            ADJ          that\n",
       "147            ADJ          busy\n",
       "155            ADJ         worth\n",
       "161            ADJ           our\n",
       "184            ADJ           our\n",
       "194            ADJ         which\n",
       "198            ADJ            my\n",
       "199            ADJ          very\n",
       "203            ADJ  disappointed\n",
       "210            ADJ           our\n",
       "217            ADJ     principle\n",
       "224            ADJ        better\n",
       "225            ADJ       quicker\n",
       "228            ADJ          taco"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjs_1star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling with Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize text\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parsed_one_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-4a4bfc25a7ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_one_star\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserver_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     return [token for sent in sentences\n\u001b[1;32m    130\u001b[0m             for token in _treebank_word_tokenizer.tokenize(sent)]\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1235\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1236\u001b[0m         \"\"\"\n\u001b[0;32m-> 1237\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m         \"\"\"\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \"\"\"\n\u001b[1;32m   1315\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[1;32m    311\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1289\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1290\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(parsed_one_star)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
