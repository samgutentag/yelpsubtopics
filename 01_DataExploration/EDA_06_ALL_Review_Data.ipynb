{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from glob import glob\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_marker(text=''):\n",
    "    print('[{}] {}'.format(datetime.datetime.now().time(), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import matplotlib\n",
    "font = {'size' : 50}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "TITLE_FONT_SIZE = 25\n",
    "LABEL_FONT_SIZE = 15\n",
    "TICK_FONT_SIZE  = 15\n",
    "\n",
    "FIG_SIZE = (15,6)\n",
    "DO_WRITE_CHARTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Review Data for Arizona Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:35:00.083347] Loading Review Data...\n",
      "[10:35:00.084824] Reading 1 of 1 ../clean_data/az_restaurant_reviews.csv...\n",
      "[10:35:07.238830] merging to dataframe...\n",
      "[10:35:08.293998] reseting index...\n",
      "[10:35:08.301953] Complete!\n"
     ]
    }
   ],
   "source": [
    "time_marker(text='Loading Review Data...')\n",
    "\n",
    "reviews = pd.DataFrame()\n",
    "file_path_slug = '../clean_data/az_restaurant_reviews.csv'\n",
    "file_list = glob(file_path_slug)\n",
    "\n",
    "# Chunk Settings\n",
    "chunks = list()\n",
    "chunksize = 10000\n",
    "for ii, file in enumerate(sorted(file_list)):\n",
    "    time_marker('Reading {} of {} {}...'.format(ii+1, len(file_list), file))\n",
    "    # import file in chunks\n",
    "    for jj, chunk in enumerate(pd.read_csv(file, chunksize=chunksize, iterator=True, index_col=0, parse_dates=['date'])):\n",
    "        \n",
    "        # append chunk to chunks list\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "time_marker(text='merging to dataframe...')\n",
    "reviews = pd.concat(chunks)\n",
    "\n",
    "time_marker('reseting index...')\n",
    "reviews.reset_index(inplace=True, drop=True)\n",
    "time_marker(text='Complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:35:08.338018] Dropping records with NaN values...\n",
      "[10:35:09.071144] Cleaning data types...\n",
      "[10:35:09.593488] assiging 'Positive' or 'Negative' classification to reviews...\n"
     ]
    }
   ],
   "source": [
    "time_marker('Dropping records with NaN values...')\n",
    "reviews.dropna(how='any', inplace=True)\n",
    "reviews.reset_index(inplace=True, drop=True)\n",
    "\n",
    "time_marker('Cleaning data types...')\n",
    "reviews['cool'] = reviews['cool'].astype('int')\n",
    "reviews['funny'] = reviews['funny'].astype('int')\n",
    "reviews['stars'] = reviews['stars'].astype('int')\n",
    "reviews['useful'] = reviews['useful'].astype('int')\n",
    "reviews['review_len'] = reviews['review_len'].astype('int')\n",
    "reviews['is_fast_food'] = reviews['is_fast_food'].apply(lambda x: True if x == 1 else False)\n",
    "reviews['date'] = pd.to_datetime(reviews['date'])\n",
    "\n",
    "time_marker('assiging \\'Positive\\' or \\'Negative\\' classification to reviews...')\n",
    "reviews['is_positive'] = reviews.stars.apply(lambda x: True if x > 3 else False)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 495893 entries, 0 to 495892\n",
      "Data columns (total 12 columns):\n",
      "business_id     495893 non-null object\n",
      "cool            495893 non-null int64\n",
      "date            495893 non-null datetime64[ns]\n",
      "funny           495893 non-null int64\n",
      "review_id       495893 non-null object\n",
      "stars           495893 non-null int64\n",
      "text            495893 non-null object\n",
      "useful          495893 non-null int64\n",
      "user_id         495893 non-null object\n",
      "is_fast_food    495893 non-null bool\n",
      "review_len      495893 non-null int64\n",
      "is_positive     495893 non-null bool\n",
      "dtypes: bool(2), datetime64[ns](1), int64(5), object(4)\n",
      "memory usage: 38.8+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_fast_food</th>\n",
       "      <th>review_len</th>\n",
       "      <th>is_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JlNeaOymdVbE6_bubqjohg</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08-09</td>\n",
       "      <td>0</td>\n",
       "      <td>BF0ANB54sc_f-3_howQBCg</td>\n",
       "      <td>1</td>\n",
       "      <td>we always go to the chevo's in chandler which ...</td>\n",
       "      <td>3</td>\n",
       "      <td>ssuXFjkH4neiBgwv-oN4IA</td>\n",
       "      <td>False</td>\n",
       "      <td>422</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08-09</td>\n",
       "      <td>0</td>\n",
       "      <td>DbLUpPT61ykLTakknCF9CQ</td>\n",
       "      <td>1</td>\n",
       "      <td>this place is always so dirty and grimy been t...</td>\n",
       "      <td>6</td>\n",
       "      <td>ssuXFjkH4neiBgwv-oN4IA</td>\n",
       "      <td>False</td>\n",
       "      <td>111</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S-oLPRdhlyL5HAknBKTUcQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>0</td>\n",
       "      <td>z_mVLygzPn8uHp63SSCErw</td>\n",
       "      <td>4</td>\n",
       "      <td>holy portion sizes! you get a lot of bang for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>MzEnYCyZlRYQRISNMXTWIg</td>\n",
       "      <td>False</td>\n",
       "      <td>130</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny               review_id  \\\n",
       "0  JlNeaOymdVbE6_bubqjohg     0 2014-08-09      0  BF0ANB54sc_f-3_howQBCg   \n",
       "1  0Rni7ocMC_Lg2UH0lDeKMQ     0 2014-08-09      0  DbLUpPT61ykLTakknCF9CQ   \n",
       "2  S-oLPRdhlyL5HAknBKTUcQ     0 2017-11-30      0  z_mVLygzPn8uHp63SSCErw   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      1  we always go to the chevo's in chandler which ...       3   \n",
       "1      1  this place is always so dirty and grimy been t...       6   \n",
       "2      4  holy portion sizes! you get a lot of bang for ...       0   \n",
       "\n",
       "                  user_id  is_fast_food  review_len  is_positive  \n",
       "0  ssuXFjkH4neiBgwv-oN4IA         False         422        False  \n",
       "1  ssuXFjkH4neiBgwv-oN4IA         False         111        False  \n",
       "2  MzEnYCyZlRYQRISNMXTWIg         False         130         True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# terms and characters to ignore, we dont care about punctuation\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "contractions = [\"'s\", \"n't\", \"'ll\", \"'t\", \"'s\", \"'re\"]\n",
    "\n",
    "# lemma\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "exclusion_terms = list(set(set(stop_words) | set(exclude) | set(contractions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(doc):\n",
    "    ''' remove stop words, remove punctuation, and lemmatize a text document'''\n",
    "\n",
    "    # lemmatize, tokenize and remove stop words, puncuation and contractions\n",
    "    # remove non alpha tokens\n",
    "    tokens = [lemma.lemmatize(word) for word in word_tokenize(doc) if word not in exclusion_terms and word.isalpha()]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Review Corpus for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_prep(corpus=None, n_terms=5):\n",
    "    '''\n",
    "    \n",
    "        @ params:\n",
    "            corpus   : a list of \n",
    "            n_terms  : the number of top terms to preview to the console\n",
    "    \n",
    "        returns:\n",
    "            a list of 3 items\n",
    "                dictionary        :  a gensim dictionary object built from the corpus\n",
    "                corpus            :  a bag of words sparce array of corpus terms\n",
    "                total_word_count  :  a defaultdict with key word identifier in dictionary, and value the count of times that word appears in the corpus\n",
    "    \n",
    "    '''\n",
    "    if corpus == None:\n",
    "        return False  \n",
    "    else:\n",
    "        time_marker('building gensim dict...')\n",
    "        # build gensim dict, key=token, value=count\n",
    "        dictionary = Dictionary(corpus)\n",
    "        # print('dictionary Tokens to ID {}'.format(dictionary.token2id))\n",
    "\n",
    "        # create a gensim corpus\n",
    "        time_marker('building gensim corpus...')\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in clean_docs]\n",
    "        # print('gensim Corpus {}'.format(corpus[0]))\n",
    "\n",
    "        # create a defaultdict\n",
    "        total_word_count = defaultdict(int)\n",
    "\n",
    "        # loop over corpus and count the number of times each word appears\n",
    "        for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "            total_word_count[word_id] += word_count\n",
    "\n",
    "        # create a sorted list from the defaultdict\n",
    "        sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
    "\n",
    "        # print top n_terms words across all documents\n",
    "        print('Top {:d} words across all documents'.format(n_terms))\n",
    "        for word_id, word_count in sorted_word_count[:n_terms]:\n",
    "            print('{:20}{}'.format(dictionary.get(word_id), word_count))\n",
    "        \n",
    "        return [dictionary, corpus, total_word_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split up by `Fast Food` and `Non Fast Food` Restaurant Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_reviews = reviews[reviews.is_fast_food == True].copy()\n",
    "ff_reviews.reset_index(inplace=True, drop=True)\n",
    "nff_reviews = reviews[reviews.is_fast_food == False].copy()\n",
    "nff_reviews.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Fast Food Reviews ==============================\n",
      "Number of Fast Food Reviews                  39907\t8.0475\n",
      "Number of Positive Fast Food Reviews         19488\t48.8335\n",
      "Number of Negative Fast Food Reviews         20419\t51.1665\n",
      "\n",
      "============================ Non Fast Food Reviews ============================\n",
      "Number of Non Fast Food Reviews              455986\t91.9525\n",
      "Number of Positive Non Fast Food Reviews     303285\t66.5119\n",
      "Number of Negative Non Fast Food Reviews     152701\t33.4881\n"
     ]
    }
   ],
   "source": [
    "print('============================== Fast Food Reviews ==============================')\n",
    "print('{:45}{:d}\\t{:2.4f}'.format('Number of Fast Food Reviews', ff_reviews.shape[0], 100.*ff_reviews.shape[0] / reviews.shape[0]))\n",
    "print('{:45}{:d}\\t{:2.4f}'.format('Number of Positive Fast Food Reviews', ff_reviews[ff_reviews.is_positive == True].shape[0], (100.*ff_reviews[ff_reviews.is_positive == True].shape[0]/ff_reviews.shape[0])))\n",
    "print('{:45}{:d}\\t{:2.4f}'.format('Number of Negative Fast Food Reviews', ff_reviews[ff_reviews.is_positive == False].shape[0], (100.*ff_reviews[ff_reviews.is_positive == False].shape[0]/ff_reviews.shape[0])))\n",
    "\n",
    "print()\n",
    "print('============================ Non Fast Food Reviews ============================')\n",
    "print('{:45}{:d}\\t{:2.4f}'.format('Number of Non Fast Food Reviews', nff_reviews.shape[0], 100.*nff_reviews.shape[0] / reviews.shape[0]))\n",
    "print('{:45}{:d}\\t{:2.4f}'.format('Number of Positive Non Fast Food Reviews', nff_reviews[nff_reviews.is_positive == True].shape[0], (100.*nff_reviews[nff_reviews.is_positive == True].shape[0]/nff_reviews.shape[0])))\n",
    "print('{:45}{:d}\\t{:2.4f}'.format('Number of Negative Non Fast Food Reviews', nff_reviews[nff_reviews.is_positive == False].shape[0], (100.*nff_reviews[nff_reviews.is_positive == False].shape[0]/nff_reviews.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Fast Food Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Step 1: </b>Subset to only evaluate `Non Fast Food` Reviews</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_fast_food</th>\n",
       "      <th>review_len</th>\n",
       "      <th>is_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JlNeaOymdVbE6_bubqjohg</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08-09</td>\n",
       "      <td>0</td>\n",
       "      <td>BF0ANB54sc_f-3_howQBCg</td>\n",
       "      <td>1</td>\n",
       "      <td>we always go to the chevo's in chandler which ...</td>\n",
       "      <td>3</td>\n",
       "      <td>ssuXFjkH4neiBgwv-oN4IA</td>\n",
       "      <td>False</td>\n",
       "      <td>422</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08-09</td>\n",
       "      <td>0</td>\n",
       "      <td>DbLUpPT61ykLTakknCF9CQ</td>\n",
       "      <td>1</td>\n",
       "      <td>this place is always so dirty and grimy been t...</td>\n",
       "      <td>6</td>\n",
       "      <td>ssuXFjkH4neiBgwv-oN4IA</td>\n",
       "      <td>False</td>\n",
       "      <td>111</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S-oLPRdhlyL5HAknBKTUcQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>0</td>\n",
       "      <td>z_mVLygzPn8uHp63SSCErw</td>\n",
       "      <td>4</td>\n",
       "      <td>holy portion sizes! you get a lot of bang for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>MzEnYCyZlRYQRISNMXTWIg</td>\n",
       "      <td>False</td>\n",
       "      <td>130</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny               review_id  \\\n",
       "0  JlNeaOymdVbE6_bubqjohg     0 2014-08-09      0  BF0ANB54sc_f-3_howQBCg   \n",
       "1  0Rni7ocMC_Lg2UH0lDeKMQ     0 2014-08-09      0  DbLUpPT61ykLTakknCF9CQ   \n",
       "2  S-oLPRdhlyL5HAknBKTUcQ     0 2017-11-30      0  z_mVLygzPn8uHp63SSCErw   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      1  we always go to the chevo's in chandler which ...       3   \n",
       "1      1  this place is always so dirty and grimy been t...       6   \n",
       "2      4  holy portion sizes! you get a lot of bang for ...       0   \n",
       "\n",
       "                  user_id  is_fast_food  review_len  is_positive  \n",
       "0  ssuXFjkH4neiBgwv-oN4IA         False         422        False  \n",
       "1  ssuXFjkH4neiBgwv-oN4IA         False         111        False  \n",
       "2  MzEnYCyZlRYQRISNMXTWIg         False         130         True  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495893"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Step 2: </b>Tokenize and Normalize review text</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:35:11.971620] tokenizing and normalizing text...\n",
      "[10:54:11.808859] done!\n"
     ]
    }
   ],
   "source": [
    "time_marker('tokenizing and normalizing text...')\n",
    "reviews['tokens'] = reviews.text.apply(lambda r: clean_review(r))\n",
    "reviews['norm_text'] = reviews.tokens.apply(lambda t: ' '.join(t))\n",
    "time_marker('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strip down to top 10,000 most frequent tokens\n",
    "<p>Well not exactly top 10,000 keep terms that appear more frequently than the 10,000th term does.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lists = list(reviews['tokens'].values)\n",
    "flat_list = [item for sublist in token_lists for item in sublist]\n",
    "token_counter = Counter(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Unique Token Count   112123\n",
      "Pruned Token Count            9967\n"
     ]
    }
   ],
   "source": [
    "term_keep_count = 10000\n",
    "\n",
    "title_text = 'Reviews'\n",
    "\n",
    "df = pd.DataFrame.from_dict(token_counter, orient='index').reset_index()\n",
    "df.columns = ['token', 'token_count']\n",
    "df.sort_values('token_count', ascending=False, inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "ax = df['token_count'].plot(logy=True, figsize=FIG_SIZE, marker='.', linestyle='')\n",
    "ax.set_title('Token Frequency in Corpus {}'.format(title_text.title()), size=TITLE_FONT_SIZE)\n",
    "ax.set_xlabel('Token', size=LABEL_FONT_SIZE)\n",
    "ax.set_ylabel('Token Count', size=LABEL_FONT_SIZE)\n",
    "ax.set_xticks([])\n",
    "\n",
    "# cutoff lines\n",
    "ax.axvline(term_keep_count)\n",
    "ax.axhline(df.iloc[term_keep_count,1])\n",
    "\n",
    "if DO_WRITE_CHARTS:\n",
    "    plt.savefig('../charts/token_dist/tokens_count_curve_ALLTOKENS_{}.png'.format(title_text.lower().replace(' ', '_')))\n",
    "else:\n",
    "    plt.show()\n",
    "plt.close()\n",
    "    \n",
    "# going to drop tokens that appear fewer than 4 times\n",
    "print('{:30}{:d}'.format('Original Unique Token Count', df.shape[0]))\n",
    "print('{:30}{:d}'.format('Pruned Token Count', df[df.token_count > df.iloc[term_keep_count,1]].shape[0]))\n",
    "\n",
    "tokens_to_keep = list(df[df.token_count > df.iloc[term_keep_count,1]].token.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune tokens not found in `tokens_to_keep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['tokens_keep'] = reviews['tokens'].apply(lambda token_list: [token for token in token_list if token in tokens_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_token_changes(df, title_text):\n",
    "\n",
    "    diffs = df[df.tokens.str.len() != df.tokens_keep.str.len()].copy()\n",
    "    origs = df[df.tokens.str.len() == df.tokens_keep.str.len()].copy()\n",
    "\n",
    "    plt.subplots(figsize=(10, 10))\n",
    "    plt.scatter(x=diffs.tokens.str.len(), y=diffs.tokens_keep.str.len(), color='b')\n",
    "    plt.scatter(x=origs.tokens.str.len(), y=origs.tokens_keep.str.len(), color='g', alpha=0.5)\n",
    "\n",
    "    plt.legend(['Token Count Changed', 'All Tokens Kept'], frameon=True, loc=2)\n",
    "    plt.title('Token Count Changes {}'.format(title_text.title()), size=TITLE_FONT_SIZE)\n",
    "    plt.xlabel('Original Tokens Count', size=LABEL_FONT_SIZE)\n",
    "    plt.ylabel('Cleaned Tokens Count', size=LABEL_FONT_SIZE)\n",
    "\n",
    "    if DO_WRITE_CHARTS:\n",
    "        plt.savefig('../charts/token_dist/tokens_lost_diff_{}.png'.format(title_text.lower().replace(' ', '_')))\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    plt.subplots(figsize=FIG_SIZE)\n",
    "    df['tokens_lost'] = df.tokens.str.len() - df.tokens_keep.str.len() \n",
    "\n",
    "    df.tokens_lost.plot.hist(color='r', bins=100)\n",
    "    plt.title('Lost Token Distribution {}'.format(title_text.title()), size=TITLE_FONT_SIZE)\n",
    "    plt.xlabel('Number of Tokens Removed', size=LABEL_FONT_SIZE)\n",
    "    plt.ylabel('Frequency', size=LABEL_FONT_SIZE)\n",
    "    \n",
    "    if DO_WRITE_CHARTS:\n",
    "        plt.savefig('../charts/token_dist/tokens_lost_dist_{}.png'.format(title_text.lower().replace(' ', '_')))\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "reviews = plot_token_changes(reviews, 'All Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <td>JlNeaOymdVbE6_bubqjohg</td>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>S-oLPRdhlyL5HAknBKTUcQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cool</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>2014-08-09 00:00:00</td>\n",
       "      <td>2014-08-09 00:00:00</td>\n",
       "      <td>2017-11-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <td>BF0ANB54sc_f-3_howQBCg</td>\n",
       "      <td>DbLUpPT61ykLTakknCF9CQ</td>\n",
       "      <td>z_mVLygzPn8uHp63SSCErw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>we always go to the chevo's in chandler which ...</td>\n",
       "      <td>this place is always so dirty and grimy been t...</td>\n",
       "      <td>holy portion sizes! you get a lot of bang for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>ssuXFjkH4neiBgwv-oN4IA</td>\n",
       "      <td>ssuXFjkH4neiBgwv-oN4IA</td>\n",
       "      <td>MzEnYCyZlRYQRISNMXTWIg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_fast_food</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_len</th>\n",
       "      <td>422</td>\n",
       "      <td>111</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_positive</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>[always, go, chevo, chandler, delicious, one, ...</td>\n",
       "      <td>[place, always, dirty, grimy, twice, back, cus...</td>\n",
       "      <td>[holy, portion, size, get, lot, bang, buck, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_text</th>\n",
       "      <td>always go chevo chandler delicious one ahwatuk...</td>\n",
       "      <td>place always dirty grimy twice back customer s...</td>\n",
       "      <td>holy portion size get lot bang buck service su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_keep</th>\n",
       "      <td>[always, go, chandler, delicious, one, ahwatuk...</td>\n",
       "      <td>[place, always, dirty, grimy, twice, back, cus...</td>\n",
       "      <td>[holy, portion, size, get, lot, bang, buck, se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_lost</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              0  \\\n",
       "business_id                              JlNeaOymdVbE6_bubqjohg   \n",
       "cool                                                          0   \n",
       "date                                        2014-08-09 00:00:00   \n",
       "funny                                                         0   \n",
       "review_id                                BF0ANB54sc_f-3_howQBCg   \n",
       "stars                                                         1   \n",
       "text          we always go to the chevo's in chandler which ...   \n",
       "useful                                                        3   \n",
       "user_id                                  ssuXFjkH4neiBgwv-oN4IA   \n",
       "is_fast_food                                              False   \n",
       "review_len                                                  422   \n",
       "is_positive                                               False   \n",
       "tokens        [always, go, chevo, chandler, delicious, one, ...   \n",
       "norm_text     always go chevo chandler delicious one ahwatuk...   \n",
       "tokens_keep   [always, go, chandler, delicious, one, ahwatuk...   \n",
       "tokens_lost                                                   1   \n",
       "\n",
       "                                                              1  \\\n",
       "business_id                              0Rni7ocMC_Lg2UH0lDeKMQ   \n",
       "cool                                                          0   \n",
       "date                                        2014-08-09 00:00:00   \n",
       "funny                                                         0   \n",
       "review_id                                DbLUpPT61ykLTakknCF9CQ   \n",
       "stars                                                         1   \n",
       "text          this place is always so dirty and grimy been t...   \n",
       "useful                                                        6   \n",
       "user_id                                  ssuXFjkH4neiBgwv-oN4IA   \n",
       "is_fast_food                                              False   \n",
       "review_len                                                  111   \n",
       "is_positive                                               False   \n",
       "tokens        [place, always, dirty, grimy, twice, back, cus...   \n",
       "norm_text     place always dirty grimy twice back customer s...   \n",
       "tokens_keep   [place, always, dirty, grimy, twice, back, cus...   \n",
       "tokens_lost                                                   0   \n",
       "\n",
       "                                                              2  \n",
       "business_id                              S-oLPRdhlyL5HAknBKTUcQ  \n",
       "cool                                                          0  \n",
       "date                                        2017-11-30 00:00:00  \n",
       "funny                                                         0  \n",
       "review_id                                z_mVLygzPn8uHp63SSCErw  \n",
       "stars                                                         4  \n",
       "text          holy portion sizes! you get a lot of bang for ...  \n",
       "useful                                                        0  \n",
       "user_id                                  MzEnYCyZlRYQRISNMXTWIg  \n",
       "is_fast_food                                              False  \n",
       "review_len                                                  130  \n",
       "is_positive                                                True  \n",
       "tokens        [holy, portion, size, get, lot, bang, buck, se...  \n",
       "norm_text     holy portion size get lot bang buck service su...  \n",
       "tokens_keep   [holy, portion, size, get, lot, bang, buck, se...  \n",
       "tokens_lost                                                   0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(3).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Step 3: </b>Build our term dictionary, document term matrix, and preview the most common terms</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:05:20.295657] building gensim dict...\n",
      "[11:06:08.563684] building gensim corpus...\n",
      "Top 25 words across all documents\n",
      "food                404223\n",
      "place               321468\n",
      "good                311094\n",
      "great               224438\n",
      "time                203126\n",
      "service             190501\n",
      "like                182615\n",
      "one                 155133\n",
      "get                 154615\n",
      "back                150301\n",
      "go                  140685\n",
      "would               140516\n",
      "restaurant          133011\n",
      "order               131533\n",
      "really              126350\n",
      "pizza               123077\n",
      "chicken             117052\n",
      "ordered             113399\n",
      "u                   100542\n",
      "love                96393\n",
      "got                 95835\n",
      "also                94927\n",
      "best                93182\n",
      "always              89485\n",
      "try                 87709\n"
     ]
    }
   ],
   "source": [
    "# collect all cleaned review strings into a list of strings\n",
    "clean_docs = list(reviews.tokens_keep.values)\n",
    "\n",
    "# create dictionary, corpus, and word counts with custom function\n",
    "dictionary, doc_term_matrix, total_word_count = lda_prep(corpus=clean_docs, n_terms=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Step 4: </b>Using a Multicore LDA model, attempt to identify topics from the dictionary</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "ldam = LdaMulticore\n",
    "\n",
    "num_topics = 50\n",
    "num_words  = 10\n",
    "num_passes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:07:04.383644] started generating lda multicore model\n",
      "[02:43:09.580404] done!\n"
     ]
    }
   ],
   "source": [
    "time_marker('started generating lda multicore model')\n",
    "ldam_model = ldam(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=num_passes)\n",
    "time_marker('done!')\n",
    "\n",
    "results = ldam_model.print_topics(num_topics=num_topics, num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Step 6: </b>View Results</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Model Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_terms(model, num_topics=num_topics, num_words=10, unique=False):\n",
    "    results = model.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "    if not unique:\n",
    "        print('=============================== Terms Per Topic ===============================')\n",
    "        for r in results:\n",
    "            topic = r[0]\n",
    "            term_list = r[1]\n",
    "\n",
    "            term_list = term_list.split('\"')[1::2]\n",
    "            topic_terms = [term for term in term_list]\n",
    "            print('{}\\t{}'.format(topic, topic_terms))\n",
    "    else:\n",
    "        terms = [x[1] for x in results]\n",
    "        term_lists = [x.split('\"')[1::2] for x in terms]\n",
    "\n",
    "        flatList = itertools.chain.from_iterable(term_lists)\n",
    "        term_counts = Counter(flatList)\n",
    "\n",
    "        # non_unique_terms = term_counts\n",
    "        test = dict(term_counts)\n",
    "\n",
    "        # extract terms that appear more than once\n",
    "        non_unique_terms = [key for key, value in test.items() if value > 1]\n",
    "        \n",
    "        \n",
    "        print('============================ Unique Terms Per Topic ===========================')\n",
    "        for r in results:\n",
    "            topic = r[0]\n",
    "            term_list = r[1]\n",
    "\n",
    "            term_list = term_list.split('\"')[1::2]\n",
    "            topic_terms = [term for term in term_list if term not in non_unique_terms]\n",
    "            print('{}\\t{}'.format(topic, topic_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================== Terms Per Topic ===============================\n",
      "0\t['dessert', 'wine', 'ice', 'cream', 'cake', 'entree', 'meal', 'chocolate', 'course', 'appetizer']\n",
      "1\t['time', 'back', 'first', 'try', 'place', 'went', 'definitely', 'go', 'next', 'great']\n",
      "2\t['u', 'table', 'came', 'server', 'food', 'drink', 'waitress', 'asked', 'ordered', 'minute']\n",
      "3\t['dish', 'flavor', 'sauce', 'like', 'taste', 'menu', 'one', 'would', 'bit', 'meat']\n",
      "4\t['pizza', 'crust', 'slice', 'topping', 'cheese', 'pie', 'thin', 'good', 'sauce', 'pepperoni']\n",
      "5\t['crab', 'leg', 'shell', 'pound', 'coworker', 'saving', 'panini', 'hub', 'e', 'angry']\n",
      "6\t['chicken', 'rice', 'chinese', 'fried', 'food', 'beef', 'soup', 'egg', 'orange', 'sour']\n",
      "7\t['wait', 'minute', 'time', 'food', 'get', 'order', 'long', 'line', 'hour', 'waiting']\n",
      "8\t['great', 'nice', 'patio', 'atmosphere', 'outside', 'place', 'cool', 'fun', 'inside', 'food']\n",
      "9\t['order', 'ordered', 'called', 'delivery', 'extra', 'got', 'time', 'get', 'card', 'call']\n",
      "10\t['tempe', 'school', 'opening', 'mill', 'w', 'college', 'b', 'asu', 'high', 'grand']\n",
      "11\t['fresh', 'ingredient', 'olive', 'tomato', 'salad', 'oil', 'made', 'bianco', 'mozzarella', 'basil']\n",
      "12\t['best', 'year', 'place', 'restaurant', 'phoenix', 'one', 'ever', 'food', 'valley', 'family']\n",
      "13\t['table', 'dirty', 'clean', 'bathroom', 'floor', 'plate', 'hand', 'cup', 'paper', 'chair']\n",
      "14\t['chicken', 'fried', 'waffle', 'deep', 'breast', 'piece', 'cajun', 'tender', 'catfish', 'crispy']\n",
      "15\t['mouth', 'smell', 'taste', 'word', 'world', 'heaven', 'bud', 'holy', 'win', 'watering']\n",
      "16\t['always', 'love', 'place', 'get', 'time', 'food', 'go', 'favorite', 'great', 'never']\n",
      "17\t['food', 'experience', 'service', 'great', 'server', 'u', 'staff', 'friendly', 'restaurant', 'made']\n",
      "18\t['sushi', 'roll', 'tuna', 'fresh', 'chef', 'salmon', 'japanese', 'spicy', 'tempura', 'fish']\n",
      "19\t['option', 'menu', 'free', 'gyro', 'meat', 'vegetarian', 'veggie', 'choose', 'vegan', 'choice']\n",
      "20\t['food', 'like', 'place', 'ordered', 'tasted', 'bad', 'even', 'back', 'cold', 'taste']\n",
      "21\t['good', 'food', 'place', 'price', 'pretty', 'service', 'better', 'like', 'would', 'really']\n",
      "22\t['burger', 'fry', 'onion', 'bun', 'shake', 'ring', 'cheese', 'french', 'bacon', 'patty']\n",
      "23\t['bowl', 'pho', 'soup', 'noodle', 'roll', 'spring', 'broth', 'vietnamese', 'beef', 'rice']\n",
      "24\t['star', 'review', 'give', 'yelp', 'five', 'reason', 'rating', 'giving', 'based', 'shot']\n",
      "25\t['location', 'drive', 'employee', 'thru', 'window', 'car', 'fast', 'chipotle', 'dont', 'im']\n",
      "26\t['restaurant', 'parking', 'menu', 'lunch', 'small', 'lot', 'area', 'find', 'little', 'place']\n",
      "27\t['great', 'food', 'service', 'place', 'friendly', 'good', 'recommend', 'staff', 'delicious', 'price']\n",
      "28\t['wing', 'pasta', 'sauce', 'italian', 'garlic', 'bread', 'meatball', 'spaghetti', 'dish', 'ranch']\n",
      "29\t['steak', 'cooked', 'potato', 'perfectly', 'well', 'medium', 'rare', 'filet', 'mashed', 'perfection']\n",
      "30\t['bar', 'room', 'dining', 'bartender', 'area', 'sit', 'drink', 'airport', 'sat', 'table']\n",
      "31\t['breakfast', 'egg', 'coffee', 'pancake', 'bacon', 'morning', 'toast', 'brunch', 'biscuit', 'gravy']\n",
      "32\t['fish', 'shrimp', 'ramen', 'chip', 'fried', 'sea', 'ceviche', 'ahi', 'sauce', 'mahi']\n",
      "33\t['hot', 'dog', 'chicago', 'beef', 'chili', 'piping', 'italian', 'slider', 'wendy', 'pepper']\n",
      "34\t['place', 'like', 'get', 'know', 'go', 'one', 'say', 'want', 'make', 'really']\n",
      "35\t['good', 'really', 'got', 'ordered', 'little', 'nice', 'pretty', 'came', 'portion', 'also']\n",
      "36\t['sandwich', 'bread', 'sub', 'cheese', 'turkey', 'meat', 'subway', 'shop', 'lunch', 'philly']\n",
      "37\t['seafood', 'lobster', 'shrimp', 'pot', 'calamari', 'chop', 'sum', 'duck', 'dim', 'clam']\n",
      "38\t['taco', 'mexican', 'salsa', 'burrito', 'chip', 'bean', 'tortilla', 'food', 'carne', 'asada']\n",
      "39\t['bbq', 'pork', 'rib', 'meat', 'brisket', 'sauce', 'pulled', 'side', 'good', 'corn']\n",
      "40\t['salad', 'chicken', 'pita', 'dressing', 'greek', 'hummus', 'lettuce', 'wrap', 'fresh', 'lunch']\n",
      "41\t['kid', 'music', 'loud', 'wall', 'game', 'child', 'hole', 'playing', 'play', 'family']\n",
      "42\t['cheese', 'sweet', 'delicious', 'sauce', 'perfect', 'mac', 'onion', 'green', 'potato', 'pepper']\n",
      "43\t['happy', 'hour', 'beer', 'drink', 'selection', 'great', 'menu', 'special', 'price', 'deal']\n",
      "44\t['customer', 'manager', 'service', 'back', 'food', 'said', 'time', 'asked', 'one', 'would']\n",
      "45\t['buffet', 'indian', 'food', 'lamb', 'chicken', 'dish', 'naan', 'rice', 'masala', 'restaurant']\n",
      "46\t['son', 'sister', 'mother', 'brother', 'http', 'father', 'c', 'buck', 'law', 'wallet']\n",
      "47\t['night', 'dinner', 'last', 'party', 'friday', 'went', 'reservation', 'birthday', 'group', 'saturday']\n",
      "48\t['thai', 'tea', 'spicy', 'curry', 'pad', 'tofu', 'iced', 'dish', 'spice', 'rice']\n",
      "49\t['grill', 'cookie', 'hawaiian', 'cooky', 'mini', 'macaroni', 'island', 'paradise', 'scoop', 'nugget']\n"
     ]
    }
   ],
   "source": [
    "print_topic_terms(ldam_model, num_topics=num_topics, num_words=10, unique=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================ Unique Terms Per Topic ===========================\n",
      "0\t['dessert', 'wine', 'ice', 'cream', 'cake', 'entree', 'meal', 'chocolate', 'course', 'appetizer']\n",
      "1\t['first', 'try', 'definitely', 'next']\n",
      "2\t['waitress']\n",
      "3\t['flavor', 'bit']\n",
      "4\t['pizza', 'crust', 'slice', 'topping', 'pie', 'thin', 'pepperoni']\n",
      "5\t['crab', 'leg', 'shell', 'pound', 'coworker', 'saving', 'panini', 'hub', 'e', 'angry']\n",
      "6\t['chinese', 'orange', 'sour']\n",
      "7\t['wait', 'long', 'line', 'waiting']\n",
      "8\t['patio', 'atmosphere', 'outside', 'cool', 'fun', 'inside']\n",
      "9\t['called', 'delivery', 'extra', 'card', 'call']\n",
      "10\t['tempe', 'school', 'opening', 'mill', 'w', 'college', 'b', 'asu', 'high', 'grand']\n",
      "11\t['ingredient', 'olive', 'tomato', 'oil', 'bianco', 'mozzarella', 'basil']\n",
      "12\t['best', 'year', 'phoenix', 'ever', 'valley']\n",
      "13\t['dirty', 'clean', 'bathroom', 'floor', 'plate', 'hand', 'cup', 'paper', 'chair']\n",
      "14\t['waffle', 'deep', 'breast', 'piece', 'cajun', 'tender', 'catfish', 'crispy']\n",
      "15\t['mouth', 'smell', 'word', 'world', 'heaven', 'bud', 'holy', 'win', 'watering']\n",
      "16\t['always', 'love', 'favorite', 'never']\n",
      "17\t['experience']\n",
      "18\t['sushi', 'tuna', 'chef', 'salmon', 'japanese', 'tempura']\n",
      "19\t['option', 'free', 'gyro', 'vegetarian', 'veggie', 'choose', 'vegan', 'choice']\n",
      "20\t['tasted', 'bad', 'even', 'cold']\n",
      "21\t['better']\n",
      "22\t['burger', 'fry', 'bun', 'shake', 'ring', 'french', 'patty']\n",
      "23\t['bowl', 'pho', 'noodle', 'spring', 'broth', 'vietnamese']\n",
      "24\t['star', 'review', 'give', 'yelp', 'five', 'reason', 'rating', 'giving', 'based', 'shot']\n",
      "25\t['location', 'drive', 'employee', 'thru', 'window', 'car', 'fast', 'chipotle', 'dont', 'im']\n",
      "26\t['parking', 'small', 'lot', 'find']\n",
      "27\t['recommend']\n",
      "28\t['wing', 'pasta', 'garlic', 'meatball', 'spaghetti', 'ranch']\n",
      "29\t['steak', 'cooked', 'perfectly', 'well', 'medium', 'rare', 'filet', 'mashed', 'perfection']\n",
      "30\t['bar', 'room', 'dining', 'bartender', 'sit', 'airport', 'sat']\n",
      "31\t['breakfast', 'coffee', 'pancake', 'morning', 'toast', 'brunch', 'biscuit', 'gravy']\n",
      "32\t['ramen', 'sea', 'ceviche', 'ahi', 'mahi']\n",
      "33\t['hot', 'dog', 'chicago', 'chili', 'piping', 'slider', 'wendy']\n",
      "34\t['know', 'say', 'want', 'make']\n",
      "35\t['portion', 'also']\n",
      "36\t['sandwich', 'sub', 'turkey', 'subway', 'shop', 'philly']\n",
      "37\t['seafood', 'lobster', 'pot', 'calamari', 'chop', 'sum', 'duck', 'dim', 'clam']\n",
      "38\t['taco', 'mexican', 'salsa', 'burrito', 'bean', 'tortilla', 'carne', 'asada']\n",
      "39\t['bbq', 'pork', 'rib', 'brisket', 'pulled', 'side', 'corn']\n",
      "40\t['pita', 'dressing', 'greek', 'hummus', 'lettuce', 'wrap']\n",
      "41\t['kid', 'music', 'loud', 'wall', 'game', 'child', 'hole', 'playing', 'play']\n",
      "42\t['sweet', 'perfect', 'mac', 'green']\n",
      "43\t['happy', 'beer', 'selection', 'special', 'deal']\n",
      "44\t['customer', 'manager', 'said']\n",
      "45\t['buffet', 'indian', 'lamb', 'naan', 'masala']\n",
      "46\t['son', 'sister', 'mother', 'brother', 'http', 'father', 'c', 'buck', 'law', 'wallet']\n",
      "47\t['night', 'dinner', 'last', 'party', 'friday', 'reservation', 'birthday', 'group', 'saturday']\n",
      "48\t['thai', 'tea', 'curry', 'pad', 'tofu', 'iced', 'spice']\n",
      "49\t['grill', 'cookie', 'hawaiian', 'cooky', 'mini', 'macaroni', 'island', 'paradise', 'scoop', 'nugget']\n"
     ]
    }
   ],
   "source": [
    "print_topic_terms(ldam_model, num_topics=num_topics, num_words=10, unique=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk (no need to use pickle module)\n",
    "term = 'all_restaurants'\n",
    "file_suffix = '{}_{:d}_topics_{:d}_terms_{}_passes'.format(term, num_topics, num_words, num_passes)\n",
    "ldam_model.save('../models/ldam_{}.model'.format(file_suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:43:10.482795] Done!\n"
     ]
    }
   ],
   "source": [
    "time_marker('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
