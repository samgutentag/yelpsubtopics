{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review Data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from glob import glob\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_marker(text=''):\n",
    "    print('[{}] {}'.format(datetime.datetime.now().time(), text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
    "\n",
    "import matplotlib\n",
    "font = {'size' : 50}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "TITLE_FONT_SIZE = 25\n",
    "LABEL_FONT_SIZE = 15\n",
    "TICK_FONT_SIZE  = 15\n",
    "\n",
    "FIG_SIZE = (15,6)\n",
    "DO_WRITE_CHARTS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Review Data for Arizona Restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:34:57.651551] Loading Review Data...\n",
      "[09:34:57.653803] Reading 1 of 1 ../clean_data/az_restaurant_reviews.csv...\n",
      "[09:35:05.383597] merging to dataframe...\n",
      "[09:35:06.584906] reseting index...\n",
      "[09:35:06.595688] Complete!\n"
     ]
    }
   ],
   "source": [
    "time_marker(text='Loading Review Data...')\n",
    "\n",
    "reviews = pd.DataFrame()\n",
    "file_path_slug = '../clean_data/az_restaurant_reviews.csv'\n",
    "file_list = glob(file_path_slug)\n",
    "\n",
    "# Chunk Settings\n",
    "chunks = list()\n",
    "chunksize = 10000\n",
    "for ii, file in enumerate(sorted(file_list)):\n",
    "    time_marker('Reading {} of {} {}...'.format(ii+1, len(file_list), file))\n",
    "    # import file in chunks\n",
    "    for jj, chunk in enumerate(pd.read_csv(file, chunksize=chunksize, iterator=True, index_col=0, parse_dates=['date'])):\n",
    "        \n",
    "        # append chunk to chunks list\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "time_marker(text='merging to dataframe...')\n",
    "reviews = pd.concat(chunks)\n",
    "\n",
    "time_marker('reseting index...')\n",
    "reviews.reset_index(inplace=True, drop=True)\n",
    "time_marker(text='Complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:35:06.639397] Dropping records with NaN values...\n",
      "[09:35:07.517161] Cleaning data types...\n",
      "[09:35:08.121576] assiging 'Positive' or 'Negative' classification to reviews...\n"
     ]
    }
   ],
   "source": [
    "time_marker('Dropping records with NaN values...')\n",
    "reviews.dropna(how='any', inplace=True)\n",
    "reviews.reset_index(inplace=True, drop=True)\n",
    "\n",
    "time_marker('Cleaning data types...')\n",
    "reviews['cool'] = reviews['cool'].astype('int')\n",
    "reviews['funny'] = reviews['funny'].astype('int')\n",
    "reviews['stars'] = reviews['stars'].astype('int')\n",
    "reviews['useful'] = reviews['useful'].astype('int')\n",
    "reviews['review_len'] = reviews['review_len'].astype('int')\n",
    "reviews['is_fast_food'] = reviews['is_fast_food'].apply(lambda x: True if x == 1 else False)\n",
    "reviews['date'] = pd.to_datetime(reviews['date'])\n",
    "\n",
    "time_marker('assiging \\'Positive\\' or \\'Negative\\' classification to reviews...')\n",
    "reviews['is_positive'] = reviews.stars.apply(lambda x: True if x > 3 else False)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 495893 entries, 0 to 495892\n",
      "Data columns (total 12 columns):\n",
      "business_id     495893 non-null object\n",
      "cool            495893 non-null int64\n",
      "date            495893 non-null datetime64[ns]\n",
      "funny           495893 non-null int64\n",
      "review_id       495893 non-null object\n",
      "stars           495893 non-null int64\n",
      "text            495893 non-null object\n",
      "useful          495893 non-null int64\n",
      "user_id         495893 non-null object\n",
      "is_fast_food    495893 non-null bool\n",
      "review_len      495893 non-null int64\n",
      "is_positive     495893 non-null bool\n",
      "dtypes: bool(2), datetime64[ns](1), int64(5), object(4)\n",
      "memory usage: 38.8+ MB\n"
     ]
    }
   ],
   "source": [
    "reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_fast_food</th>\n",
       "      <th>review_len</th>\n",
       "      <th>is_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JlNeaOymdVbE6_bubqjohg</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08-09</td>\n",
       "      <td>0</td>\n",
       "      <td>BF0ANB54sc_f-3_howQBCg</td>\n",
       "      <td>1</td>\n",
       "      <td>we always go to the chevo's in chandler which ...</td>\n",
       "      <td>3</td>\n",
       "      <td>ssuXFjkH4neiBgwv-oN4IA</td>\n",
       "      <td>False</td>\n",
       "      <td>422</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0Rni7ocMC_Lg2UH0lDeKMQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-08-09</td>\n",
       "      <td>0</td>\n",
       "      <td>DbLUpPT61ykLTakknCF9CQ</td>\n",
       "      <td>1</td>\n",
       "      <td>this place is always so dirty and grimy been t...</td>\n",
       "      <td>6</td>\n",
       "      <td>ssuXFjkH4neiBgwv-oN4IA</td>\n",
       "      <td>False</td>\n",
       "      <td>111</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S-oLPRdhlyL5HAknBKTUcQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-30</td>\n",
       "      <td>0</td>\n",
       "      <td>z_mVLygzPn8uHp63SSCErw</td>\n",
       "      <td>4</td>\n",
       "      <td>holy portion sizes! you get a lot of bang for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>MzEnYCyZlRYQRISNMXTWIg</td>\n",
       "      <td>False</td>\n",
       "      <td>130</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny               review_id  \\\n",
       "0  JlNeaOymdVbE6_bubqjohg     0 2014-08-09      0  BF0ANB54sc_f-3_howQBCg   \n",
       "1  0Rni7ocMC_Lg2UH0lDeKMQ     0 2014-08-09      0  DbLUpPT61ykLTakknCF9CQ   \n",
       "2  S-oLPRdhlyL5HAknBKTUcQ     0 2017-11-30      0  z_mVLygzPn8uHp63SSCErw   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      1  we always go to the chevo's in chandler which ...       3   \n",
       "1      1  this place is always so dirty and grimy been t...       6   \n",
       "2      4  holy portion sizes! you get a lot of bang for ...       0   \n",
       "\n",
       "                  user_id  is_fast_food  review_len  is_positive  \n",
       "0  ssuXFjkH4neiBgwv-oN4IA         False         422        False  \n",
       "1  ssuXFjkH4neiBgwv-oN4IA         False         111        False  \n",
       "2  MzEnYCyZlRYQRISNMXTWIg         False         130         True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# terms and characters to ignore, we dont care about punctuation\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "contractions = [\"'s\", \"n't\", \"'ll\", \"'t\", \"'s\", \"'re\"]\n",
    "\n",
    "# lemma\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "exclusion_terms = list(set(set(stop_words) | set(exclude) | set(contractions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(doc):\n",
    "    ''' remove stop words, remove punctuation, and lemmatize a text document'''\n",
    "\n",
    "    # lemmatize, tokenize and remove stop words, puncuation and contractions\n",
    "    # remove non alpha tokens\n",
    "    tokens = [lemma.lemmatize(word) for word in word_tokenize(doc) if word not in exclusion_terms and word.isalpha()]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Review Corpus for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from collections import defaultdict\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_prep(corpus=None, n_terms=5):\n",
    "    '''\n",
    "    \n",
    "        @ params:\n",
    "            corpus   : a list of \n",
    "            n_terms  : the number of top terms to preview to the console\n",
    "    \n",
    "        returns:\n",
    "            a list of 3 items\n",
    "                dictionary        :  a gensim dictionary object built from the corpus\n",
    "                corpus            :  a bag of words sparce array of corpus terms\n",
    "                total_word_count  :  a defaultdict with key word identifier in dictionary, and value the count of times that word appears in the corpus\n",
    "    \n",
    "    '''\n",
    "    if corpus == None:\n",
    "        return False  \n",
    "    else:\n",
    "        time_marker('building gensim dict...')\n",
    "        # build gensim dict, key=token, value=count\n",
    "        dictionary = Dictionary(corpus)\n",
    "        # print('dictionary Tokens to ID {}'.format(dictionary.token2id))\n",
    "\n",
    "        # create a gensim corpus\n",
    "        time_marker('building gensim corpus...')\n",
    "        corpus = [dictionary.doc2bow(doc) for doc in clean_docs]\n",
    "        # print('gensim Corpus {}'.format(corpus[0]))\n",
    "\n",
    "        # create a defaultdict\n",
    "        total_word_count = defaultdict(int)\n",
    "\n",
    "        # loop over corpus and count the number of times each word appears\n",
    "        for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "            total_word_count[word_id] += word_count\n",
    "\n",
    "        # create a sorted list from the defaultdict\n",
    "        sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
    "\n",
    "        # print top n_terms words across all documents\n",
    "        print('Top {:d} words across all documents'.format(n_terms))\n",
    "        for word_id, word_count in sorted_word_count[:n_terms]:\n",
    "            print('{:20}{}'.format(dictionary.get(word_id), word_count))\n",
    "        \n",
    "        return [dictionary, corpus, total_word_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split up by `Fast Food` and `Non Fast Food` Restaurant Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_reviews = reviews[reviews.is_fast_food == True].copy()\n",
    "ff_reviews.reset_index(inplace=True, drop=True)\n",
    "nff_reviews = reviews[reviews.is_fast_food == False].copy()\n",
    "nff_reviews.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================== Fast Food Reviews ==============================\n",
      "Number of Fast Food Reviews                  39907\t8.0475\n",
      "Number of Positive Fast Food Reviews         19488\t48.8335\n",
      "Number of Negative Fast Food Reviews         20419\t51.1665\n",
      "\n",
      "============================ Non Fast Food Reviews ============================\n",
      "Number of Non Fast Food Reviews              455986\t91.9525\n",
      "Number of Positive Non Fast Food Reviews     303285\t66.5119\n",
      "Number of Negative Non Fast Food Reviews     152701\t33.4881\n"
     ]
    }
   ],
   "source": [
    "print('============================== Fast Food Reviews ==============================')\n",
    "print('{:45}{:d}\\t{:2.4f}'.format('Number of Fast Food Reviews', ff_reviews.shape[0], 100.*ff_reviews.shape[0] / reviews.shape[0]))\n",
    "print('{:45}{:d}\\t{:2.4f}'.format('Number of Positive Fast Food Reviews', ff_reviews[ff_reviews.is_positive == True].shape[0], (100.*ff_reviews[ff_reviews.is_positive == True].shape[0]/ff_reviews.shape[0])))\n",
    "print('{:45}{:d}\\t{:2.4f}'.format('Number of Negative Fast Food Reviews', ff_reviews[ff_reviews.is_positive == False].shape[0], (100.*ff_reviews[ff_reviews.is_positive == False].shape[0]/ff_reviews.shape[0])))\n",
    "\n",
    "print()\n",
    "print('============================ Non Fast Food Reviews ============================')\n",
    "print('{:45}{:d}\\t{:2.4f}'.format('Number of Non Fast Food Reviews', nff_reviews.shape[0], 100.*nff_reviews.shape[0] / reviews.shape[0]))\n",
    "print('{:45}{:d}\\t{:2.4f}'.format('Number of Positive Non Fast Food Reviews', nff_reviews[nff_reviews.is_positive == True].shape[0], (100.*nff_reviews[nff_reviews.is_positive == True].shape[0]/nff_reviews.shape[0])))\n",
    "print('{:45}{:d}\\t{:2.4f}'.format('Number of Negative Non Fast Food Reviews', nff_reviews[nff_reviews.is_positive == False].shape[0], (100.*nff_reviews[nff_reviews.is_positive == False].shape[0]/nff_reviews.shape[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Food Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Step 1: </b>Subset to only evaluate `Fast Food` Reviews</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>is_fast_food</th>\n",
       "      <th>review_len</th>\n",
       "      <th>is_positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iIjVO7cLD1UEmIO7G05Ujw</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-11</td>\n",
       "      <td>0</td>\n",
       "      <td>xatycgntu_F_Ioyny3iflw</td>\n",
       "      <td>4</td>\n",
       "      <td>flavor was actually pretty good. not used to e...</td>\n",
       "      <td>0</td>\n",
       "      <td>vaXJ7-xLrnD6FAEhUqYKwQ</td>\n",
       "      <td>True</td>\n",
       "      <td>309</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8F-CalsRSKiPjjsx8ql8Lg</td>\n",
       "      <td>9</td>\n",
       "      <td>2009-12-22</td>\n",
       "      <td>8</td>\n",
       "      <td>xWvUUQ-tO-x9pAsG8JEnOQ</td>\n",
       "      <td>4</td>\n",
       "      <td>i really want to give this place four stars. g...</td>\n",
       "      <td>6</td>\n",
       "      <td>dyhTHLIf6eWBvU78Y3T06A</td>\n",
       "      <td>True</td>\n",
       "      <td>2349</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tOUFYUVuhdvtHVSrYu2hwA</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-05-07</td>\n",
       "      <td>3</td>\n",
       "      <td>uDzIp-k19kdAYM3Az9PYMw</td>\n",
       "      <td>3</td>\n",
       "      <td>i know. you are reading this and wanting to ri...</td>\n",
       "      <td>1</td>\n",
       "      <td>5V8eXkTJb6IejJkMDaj_Bw</td>\n",
       "      <td>True</td>\n",
       "      <td>598</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool       date  funny               review_id  \\\n",
       "0  iIjVO7cLD1UEmIO7G05Ujw     0 2016-06-11      0  xatycgntu_F_Ioyny3iflw   \n",
       "1  8F-CalsRSKiPjjsx8ql8Lg     9 2009-12-22      8  xWvUUQ-tO-x9pAsG8JEnOQ   \n",
       "2  tOUFYUVuhdvtHVSrYu2hwA     1 2011-05-07      3  uDzIp-k19kdAYM3Az9PYMw   \n",
       "\n",
       "   stars                                               text  useful  \\\n",
       "0      4  flavor was actually pretty good. not used to e...       0   \n",
       "1      4  i really want to give this place four stars. g...       6   \n",
       "2      3  i know. you are reading this and wanting to ri...       1   \n",
       "\n",
       "                  user_id  is_fast_food  review_len  is_positive  \n",
       "0  vaXJ7-xLrnD6FAEhUqYKwQ          True         309         True  \n",
       "1  dyhTHLIf6eWBvU78Y3T06A          True        2349         True  \n",
       "2  5V8eXkTJb6IejJkMDaj_Bw          True         598        False  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_reviews.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39907"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_reviews.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Step 2: </b>Tokenize and Normalize review text</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:35:10.912837] tokenizing and normalizing text...\n",
      "[09:36:34.214921] done!\n"
     ]
    }
   ],
   "source": [
    "time_marker('tokenizing and normalizing text...')\n",
    "ff_reviews['tokens'] = ff_reviews.text.apply(lambda r: clean_review(r))\n",
    "ff_reviews['norm_text'] = ff_reviews.tokens.apply(lambda t: ' '.join(t))\n",
    "time_marker('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strip down to top 10,000 most frequent tokens\n",
    "<p>Well not exactly top 10,000 keep terms that appear more frequently than the 10,000th term does.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_lists = list(ff_reviews['tokens'].values)\n",
    "flat_list = [item for sublist in token_lists for item in sublist]\n",
    "token_counter = Counter(flat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Unique Token Count   27669\n",
      "Pruned Token Count            9480\n"
     ]
    }
   ],
   "source": [
    "term_keep_count = 10000\n",
    "\n",
    "title_text = 'Fast Food Reviews'\n",
    "\n",
    "df = pd.DataFrame.from_dict(token_counter, orient='index').reset_index()\n",
    "df.columns = ['token', 'token_count']\n",
    "df.sort_values('token_count', ascending=False, inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "ax = df['token_count'].plot(logy=True, figsize=FIG_SIZE, marker='.', linestyle='')\n",
    "ax.set_title('Token Frequency in Corpus {}'.format(title_text.title()), size=TITLE_FONT_SIZE)\n",
    "ax.set_xlabel('Token', size=LABEL_FONT_SIZE)\n",
    "ax.set_ylabel('Token Count', size=LABEL_FONT_SIZE)\n",
    "ax.set_xticks([])\n",
    "\n",
    "# cutoff lines\n",
    "ax.axvline(term_keep_count)\n",
    "ax.axhline(df.iloc[term_keep_count,1])\n",
    "\n",
    "if DO_WRITE_CHARTS:\n",
    "    plt.savefig('../charts/token_dist/tokens_count_curve_{}.png'.format(title_text.lower().replace(' ', '_')))\n",
    "else:\n",
    "    plt.show()\n",
    "plt.close()\n",
    "    \n",
    "# going to drop tokens that appear fewer than 4 times\n",
    "print('{:30}{:d}'.format('Original Unique Token Count', df.shape[0]))\n",
    "print('{:30}{:d}'.format('Pruned Token Count', df[df.token_count > df.iloc[term_keep_count,1]].shape[0]))\n",
    "\n",
    "tokens_to_keep = list(df[df.token_count > df.iloc[term_keep_count,1]].token.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune tokens not found in `tokens_to_keep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_reviews['tokens_keep'] = ff_reviews['tokens'].apply(lambda token_list: [token for token in token_list if token in tokens_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_token_changes(df, title_text):\n",
    "\n",
    "    diffs = df[df.tokens.str.len() != df.tokens_keep.str.len()].copy()\n",
    "    origs = df[df.tokens.str.len() == df.tokens_keep.str.len()].copy()\n",
    "\n",
    "    plt.subplots(figsize=(10, 10))\n",
    "    plt.scatter(x=diffs.tokens.str.len(), y=diffs.tokens_keep.str.len(), color='b')\n",
    "    plt.scatter(x=origs.tokens.str.len(), y=origs.tokens_keep.str.len(), color='g', alpha=0.5)\n",
    "\n",
    "    plt.legend(['Token Count Changed', 'All Tokens Kept'], frameon=True, loc=2)\n",
    "    plt.title('Token Count Changes {}'.format(title_text.title()), size=TITLE_FONT_SIZE)\n",
    "    plt.xlabel('Original Tokens Count', size=LABEL_FONT_SIZE)\n",
    "    plt.ylabel('Cleaned Tokens Count', size=LABEL_FONT_SIZE)\n",
    "\n",
    "    if DO_WRITE_CHARTS:\n",
    "        plt.savefig('../charts/token_dist/tokens_lost_diff_{}.png'.format(title_text.lower().replace(' ', '_')))\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n",
    "    plt.subplots(figsize=FIG_SIZE)\n",
    "    df['tokens_lost'] = df.tokens.str.len() - df.tokens_keep.str.len() \n",
    "\n",
    "    df.tokens_lost.plot.hist(color='r', bins=100)\n",
    "    plt.title('Lost Token Distribution {}'.format(title_text.title()), size=TITLE_FONT_SIZE)\n",
    "    plt.xlabel('Number of Tokens Removed', size=LABEL_FONT_SIZE)\n",
    "    plt.ylabel('Frequency', size=LABEL_FONT_SIZE)\n",
    "    \n",
    "    if DO_WRITE_CHARTS:\n",
    "        plt.savefig('../charts/token_dist/tokens_lost_dist_{}.png'.format(title_text.lower().replace(' ', '_')))\n",
    "    else:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return df\n",
    "\n",
    "ff_reviews = plot_token_changes(ff_reviews, 'Fast Food Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <td>iIjVO7cLD1UEmIO7G05Ujw</td>\n",
       "      <td>8F-CalsRSKiPjjsx8ql8Lg</td>\n",
       "      <td>tOUFYUVuhdvtHVSrYu2hwA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cool</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>2016-06-11 00:00:00</td>\n",
       "      <td>2009-12-22 00:00:00</td>\n",
       "      <td>2011-05-07 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>funny</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_id</th>\n",
       "      <td>xatycgntu_F_Ioyny3iflw</td>\n",
       "      <td>xWvUUQ-tO-x9pAsG8JEnOQ</td>\n",
       "      <td>uDzIp-k19kdAYM3Az9PYMw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stars</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>flavor was actually pretty good. not used to e...</td>\n",
       "      <td>i really want to give this place four stars. g...</td>\n",
       "      <td>i know. you are reading this and wanting to ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>useful</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>vaXJ7-xLrnD6FAEhUqYKwQ</td>\n",
       "      <td>dyhTHLIf6eWBvU78Y3T06A</td>\n",
       "      <td>5V8eXkTJb6IejJkMDaj_Bw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_fast_food</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>review_len</th>\n",
       "      <td>309</td>\n",
       "      <td>2349</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_positive</th>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens</th>\n",
       "      <td>[flavor, actually, pretty, good, used, eating,...</td>\n",
       "      <td>[really, want, give, place, four, star, greg, ...</td>\n",
       "      <td>[know, reading, wanting, ridicule, giving, hig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>norm_text</th>\n",
       "      <td>flavor actually pretty good used eating menudo...</td>\n",
       "      <td>really want give place four star greg would st...</td>\n",
       "      <td>know reading wanting ridicule giving high rati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_keep</th>\n",
       "      <td>[flavor, actually, pretty, good, used, eating,...</td>\n",
       "      <td>[really, want, give, place, four, star, would,...</td>\n",
       "      <td>[know, reading, wanting, giving, high, rating,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tokens_lost</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              0  \\\n",
       "business_id                              iIjVO7cLD1UEmIO7G05Ujw   \n",
       "cool                                                          0   \n",
       "date                                        2016-06-11 00:00:00   \n",
       "funny                                                         0   \n",
       "review_id                                xatycgntu_F_Ioyny3iflw   \n",
       "stars                                                         4   \n",
       "text          flavor was actually pretty good. not used to e...   \n",
       "useful                                                        0   \n",
       "user_id                                  vaXJ7-xLrnD6FAEhUqYKwQ   \n",
       "is_fast_food                                               True   \n",
       "review_len                                                  309   \n",
       "is_positive                                                True   \n",
       "tokens        [flavor, actually, pretty, good, used, eating,...   \n",
       "norm_text     flavor actually pretty good used eating menudo...   \n",
       "tokens_keep   [flavor, actually, pretty, good, used, eating,...   \n",
       "tokens_lost                                                   0   \n",
       "\n",
       "                                                              1  \\\n",
       "business_id                              8F-CalsRSKiPjjsx8ql8Lg   \n",
       "cool                                                          9   \n",
       "date                                        2009-12-22 00:00:00   \n",
       "funny                                                         8   \n",
       "review_id                                xWvUUQ-tO-x9pAsG8JEnOQ   \n",
       "stars                                                         4   \n",
       "text          i really want to give this place four stars. g...   \n",
       "useful                                                        6   \n",
       "user_id                                  dyhTHLIf6eWBvU78Y3T06A   \n",
       "is_fast_food                                               True   \n",
       "review_len                                                 2349   \n",
       "is_positive                                                True   \n",
       "tokens        [really, want, give, place, four, star, greg, ...   \n",
       "norm_text     really want give place four star greg would st...   \n",
       "tokens_keep   [really, want, give, place, four, star, would,...   \n",
       "tokens_lost                                                  15   \n",
       "\n",
       "                                                              2  \n",
       "business_id                              tOUFYUVuhdvtHVSrYu2hwA  \n",
       "cool                                                          1  \n",
       "date                                        2011-05-07 00:00:00  \n",
       "funny                                                         3  \n",
       "review_id                                uDzIp-k19kdAYM3Az9PYMw  \n",
       "stars                                                         3  \n",
       "text          i know. you are reading this and wanting to ri...  \n",
       "useful                                                        1  \n",
       "user_id                                  5V8eXkTJb6IejJkMDaj_Bw  \n",
       "is_fast_food                                               True  \n",
       "review_len                                                  598  \n",
       "is_positive                                               False  \n",
       "tokens        [know, reading, wanting, ridicule, giving, hig...  \n",
       "norm_text     know reading wanting ridicule giving high rati...  \n",
       "tokens_keep   [know, reading, wanting, giving, high, rating,...  \n",
       "tokens_lost                                                   1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_reviews.head(3).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Step 3: </b>Build our term dictionary, document term matrix, and preview the most common terms</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:37:14.150609] building gensim dict...\n",
      "[09:37:17.072895] building gensim corpus...\n",
      "Top 25 words across all documents\n",
      "food                27056\n",
      "time                17393\n",
      "place               17318\n",
      "order               16877\n",
      "good                16389\n",
      "get                 15086\n",
      "like                13955\n",
      "one                 13531\n",
      "service             12699\n",
      "go                  11597\n",
      "burger              11079\n",
      "location            10850\n",
      "great               10772\n",
      "back                10626\n",
      "would               9423\n",
      "always              9420\n",
      "chicken             9300\n",
      "fry                 9055\n",
      "got                 8309\n",
      "really              7475\n",
      "sandwich            7447\n",
      "fast                7082\n",
      "even                7030\n",
      "customer            6816\n",
      "love                6795\n"
     ]
    }
   ],
   "source": [
    "# collect all cleaned review strings into a list of strings\n",
    "clean_docs = list(ff_reviews.tokens_keep.values)\n",
    "\n",
    "# create dictionary, corpus, and word counts with custom function\n",
    "dictionary, doc_term_matrix, total_word_count = lda_prep(corpus=clean_docs, n_terms=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Step 4: </b>Using a Multicore LDA model, attempt to identify topics from the dictionary</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "ldam = LdaMulticore\n",
    "\n",
    "num_topics = 20\n",
    "num_words  = 15\n",
    "num_passes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[09:37:19.897625] started generating lda multicore model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamulticore.py\", line 282, in worker_e_step\n",
      "    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamodel.py\", line 491, in do_estep\n",
      "    gamma, sstats = self.inference(chunk, collect_sstats=True)\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamodel.py\", line 458, in inference\n",
      "    Elogthetad = dirichlet_expectation(gammad)\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/site-packages/gensim/matutils.py\", line 606, in dirichlet_expectation\n",
      "    result = psi(alpha) - psi(np.sum(alpha))\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
      "    initializer(*initargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamulticore.py\", line 282, in worker_e_step\n",
      "    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamodel.py\", line 491, in do_estep\n",
      "    gamma, sstats = self.inference(chunk, collect_sstats=True)\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamodel.py\", line 458, in inference\n",
      "    Elogthetad = dirichlet_expectation(gammad)\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/site-packages/gensim/matutils.py\", line 606, in dirichlet_expectation\n",
      "    result = psi(alpha) - psi(np.sum(alpha))\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/multiprocessing/pool.py\", line 103, in worker\n",
      "    initializer(*initargs)\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamulticore.py\", line 282, in worker_e_step\n",
      "    worker_lda.do_estep(chunk)  # TODO: auto-tune alpha?\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamodel.py\", line 491, in do_estep\n",
      "    gamma, sstats = self.inference(chunk, collect_sstats=True)\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamodel.py\", line 458, in inference\n",
      "    Elogthetad = dirichlet_expectation(gammad)\n",
      "  File \"/Users/samgutentag/anaconda/envs/spring/lib/python3.6/site-packages/gensim/matutils.py\", line 606, in dirichlet_expectation\n",
      "    result = psi(alpha) - psi(np.sum(alpha))\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFull\u001b[0m                                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    243\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m                         \u001b[0mjob_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m                         \u001b[0mchunk_put\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mput\u001b[0;34m(self, obj, block, timeout)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFull\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-12a98e496e3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtime_marker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'started generating lda multicore model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mldam_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mldam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_term_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_passes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtime_marker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mldam_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, workers, chunksize, passes, batch, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, random_state, minimum_probability, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_every\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mgamma_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma_threshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminimum_probability\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m             \u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mminimum_phi_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_word_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mper_word_topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         )\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks)\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    252\u001b[0m                         \u001b[0;31m# in case the input job queue is full, keep clearing the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                         \u001b[0;31m# result queue, to make sure we don't deadlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m                         \u001b[0mprocess_result_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 \u001b[0mprocess_result_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/site-packages/gensim/models/ldamulticore.py\u001b[0m in \u001b[0;36mprocess_result_queue\u001b[0;34m(force)\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \"\"\"\n\u001b[1;32m    225\u001b[0m                 \u001b[0mmerged_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresult_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m                     \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                     \u001b[0mqueue_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mempty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/spring/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m                 \u001b[0mdeadline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_marker('started generating lda multicore model')\n",
    "ldam_model = ldam(doc_term_matrix, num_topics=num_topics, id2word=dictionary, passes=num_passes)\n",
    "time_marker('done!')\n",
    "\n",
    "results = ldam_model.print_topics(num_topics=num_topics, num_words=num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b>Step 6: </b>View Results</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print Model Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topic_terms(model, num_topics=num_topics, num_words=10, unique=False):\n",
    "    results = model.print_topics(num_topics=num_topics, num_words=num_words)\n",
    "    if not unique:\n",
    "        print('=============================== Terms Per Topic ===============================')\n",
    "        for r in results:\n",
    "            topic = r[0]\n",
    "            term_list = r[1]\n",
    "\n",
    "            term_list = term_list.split('\"')[1::2]\n",
    "            topic_terms = [term for term in term_list]\n",
    "            print('{}\\t{}'.format(topic, topic_terms))\n",
    "    else:\n",
    "        terms = [x[1] for x in results]\n",
    "        term_lists = [x.split('\"')[1::2] for x in terms]\n",
    "\n",
    "        flatList = itertools.chain.from_iterable(term_lists)\n",
    "        term_counts = Counter(flatList)\n",
    "\n",
    "        # non_unique_terms = term_counts\n",
    "        test = dict(term_counts)\n",
    "\n",
    "        # extract terms that appear more than once\n",
    "        non_unique_terms = [key for key, value in test.items() if value > 1]\n",
    "        \n",
    "        \n",
    "        print('============================ Unique Terms Per Topic ===========================')\n",
    "        for r in results:\n",
    "            topic = r[0]\n",
    "            term_list = r[1]\n",
    "\n",
    "            term_list = term_list.split('\"')[1::2]\n",
    "            topic_terms = [term for term in term_list if term not in non_unique_terms]\n",
    "            print('{}\\t{}'.format(topic, topic_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_topic_terms(ldam_model, num_topics=num_topics, num_words=10, unique=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_topic_terms(ldam_model, num_topics=num_topics, num_words=10, unique=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model to disk (no need to use pickle module)\n",
    "term = 'fast_food'\n",
    "file_suffix = '{}_{:d}_topics_{:d}_terms_{}_passes'.format(term, num_topics, num_words, num_passes)\n",
    "ldam_model.save('../models/ldam_{}.model'.format(file_suffix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_marker('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
